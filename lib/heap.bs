import io;
import memory;

struct region
{
    u8* address;
    u64 size;
}

struct allocation
{
    region* parent;
    u8* previous;
    u64 free;       // This is a bool but padded out for alignment purposes.
    u64 size;
}

// Pointer to the start of the super region, where the list of regions are
// stored.
u8* super_region = 0;

// The size of the super region. Grows as needed.
u64 super_region_size = 1024;

// The number of regions.
u64* region_count = 0;

// Where to start a search for a free allocation.
allocation* next_free = 0;

// The number of tries to find a free allocation before giving up and mapping
// in a new region.
u64 give_up_after = 2;

u64 region_min_size = 4096;

u64 align(u64 value, u64 alignment)
{
    u64 remainder = value % alignment;

    if (remainder == 0)
    {
        return value;
    }

    return value + alignment - remainder;
}

u64 get_region_index(region* r)
{
    u8* r_ptr = r;
    u64 ret = r_ptr - super_region - sizeof(u64);
    if (ret > 0)
    {
        ret = ret / sizeof(region);
    }
    return ret;
}

region* get_region(u64 index)
{
    u8* address = super_region;
    address = address + sizeof(u64);

    address = address + index * sizeof(region);

    return address;
}

u8* map(u64 len)
{
    u8* ret = syscall(9, 0, len, 3, 33, 0, 0);

    return ret;
}

// Get the minimum required space to store the given number of regions in a
// super region.
u64 super_region_min_size(u64 regions)
{
    return 8 + regions * sizeof(region);
}

void init()
{
    super_region = map(super_region_size);
    region_count = super_region;

    *region_count = 0;
}

allocation* next_allocation_in_region(allocation* current)
{
    region* reg = current.parent;
    u64 end_of_region = reg.address + reg.size;

    u64* alloc_ptr = current;
    alloc_ptr = alloc_ptr + sizeof(allocation) + current.size;

    if (alloc_ptr >= end_of_region)
    {
        return 0;
    }

    return alloc_ptr;
}

allocation* next_allocation(allocation* current)
{
    allocation* next = next_allocation_in_region(current);

    if (next != 0)
    {
        return next;
    }

    u64 index = get_region_index(current.parent);
    index = index + 1;
    if (index >= *region_count)
    {
        index = 0;
    }
    region* reg = get_region(index);
    return reg.address;
}

allocation* find_free(u64 bytes)
{
    if (*region_count == 0)
    {
        return 0;
    }

    allocation* alloc = next_free;

    // No cached starting point: start from the beginning.
    if (alloc == 0)
    {
        auto reg = get_region(0);
        alloc = reg.address;
    }

    allocation* search_start = alloc;
    bool first_run = true;
    u64 tries = give_up_after;

    while (tries > 0)
    {
        if (first_run == false && alloc == search_start)
        {
            return 0;
        }

        first_run = false;

        auto alloc_size = alloc.size;
        if (alloc.free == 1 && alloc_size >= bytes)
        {
            return alloc;
        }

        alloc = next_allocation(alloc);

        tries = tries - 1;
    }

    return 0;
}

region* allocate_region(u64 min_bytes)
{
    auto min_size = sizeof(allocation) + min_bytes;
    if (min_size < region_min_size)
    {
        min_size = region_min_size;
    }

    auto new_region_index = *region_count;
    auto new_region_count = *region_count + 1;

    // Grow the super region if necessary
    if (super_region_size < super_region_min_size(new_region_count))
    {
        super_region_size = super_region_size + super_region_size;

        u8* new_super_region = map(super_region_size);

        memory::copy(super_region, new_super_region,
                     super_region_min_size(*region_count));

        super_region = new_super_region;
        region_count = super_region;

        // TODO: free previous super region
    }

    *region_count = new_region_count;

    auto new_region = get_region(new_region_index);

    new_region.address = map(min_size);
    new_region.size = min_size;

    allocation* alloc = new_region.address;
    alloc.parent = new_region;
    alloc.previous = 0;
    alloc.free = true;
    alloc.size = new_region.size - sizeof(allocation);

    return new_region;
}

u8* allocate(u64 bytes)
{
    if (super_region == 0)
    {
        init();
    }

    bytes = align(bytes, 16);

    auto alloc = find_free(bytes);

    if (alloc == 0)
    {
        auto reg = allocate_region(bytes);
        if (reg == 0)
        {
            return 0;
        }
        alloc = reg.address;
    }

    u8* alloc_ptr = alloc;
    auto remaining = alloc.size - bytes;

    if (remaining >= sizeof(allocation))
    {
        alloc.size = bytes;

        allocation* next_alloc = alloc_ptr + sizeof(allocation) + bytes;
        next_alloc.parent = alloc.parent;
        next_alloc.previous = alloc_ptr;
        next_alloc.free = true;
        next_alloc.size = remaining - sizeof(allocation);
    }

    next_free = next_allocation(alloc);

    alloc.free = false;

    return alloc_ptr + sizeof(allocation);
}

u8* reallocate(u8* ptr, u64 bytes)
{
    bytes = align(bytes, 16);

    allocation* original_alloc = ptr - sizeof(allocation);

    // Don't bother reallocating if it's not needed.
    // TODO: shrink?
    auto original_alloc_size = original_alloc.size;
    if (original_alloc_size >= bytes)
    {
        return ptr;
    }

    allocation* new_alloc = allocate(bytes);

    memory::copy(ptr, new_alloc, original_alloc_size);

    free(ptr);

    return new_alloc;
}

void free(u8* ptr)
{
    allocation* alloc = ptr - sizeof(allocation);

    alloc.free = true;

    allocation* next = next_allocation_in_region(alloc);
    if (next != 0 && next.free == true)
    {
        alloc.size = alloc.size + sizeof(allocation) + next.size;
    }

    allocation* prev = alloc.previous;
    if (prev != 0 && prev.free == true)
    {
        prev.size = prev.size + sizeof(allocation) + alloc.size;
    }

    // Release the region if there are no more in-use allocations
    region* parent = alloc.parent;
    allocation* first = parent.address;

    if (first.free == true &&
        sizeof(allocation) + first.size == parent.size &&
        *region_count > 1)
    {
        syscall(11, parent.address, parent.size);

        u64 i = get_region_index(parent);
        u8* addr = parent;

        u64 regions = *region_count - i;
        u64 count = regions * sizeof(region);

        // TODO: make regions a linked list or something to avoid linear
        // copy on region release?
        memory::copy(addr + sizeof(region), addr, count);

        *region_count = *region_count - 1;

        return;
    }

    if (prev == 0)
    {
        next_free = alloc;
    }
    else
    {
        next_free = prev;
    }
}

// Debugging stuff ------------------------------------------------------------

u64 __debug_region_count()
{
    return *region_count;
}

u64 __debug_super_region_size()
{
    return super_region_size;
}

// Prints debugging info listing all regions and allocations
void __debug_info()
{
    u64 region_index = 0;
    while (region_index < *region_count)
    {
        auto reg = get_region(region_index);

        io::print("region %u\n", region_index);

        allocation* alloc = reg.address;

        while (alloc != 0)
        {
            if (alloc.free)
            {
                io::print("    free ");
            }
            else
            {
                io::print("    used ");
            }

            io::print("%u bytes\n", alloc.size);

            alloc = next_allocation_in_region(alloc);
        }

        region_index = region_index + 1;
    }
}
